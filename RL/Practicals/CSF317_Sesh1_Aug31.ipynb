{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu9uwKorWp6"
      },
      "source": [
        "## **Importing the libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDkL7M6krc68"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\", palette=\"deep\")\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1qc-8-xrf5I"
      },
      "source": [
        "## **Creating the plots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvSAn9eKrix1"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "  rewards = history[\"rewards\"]\n",
        "  cum_rewards = history[\"cum_rewards\"]\n",
        "  chosen_arms = history[\"arms\"]\n",
        "\n",
        "  fig = plt.figure(figsize=[30,8])\n",
        "\n",
        "  ax2 = fig.add_subplot(121)\n",
        "  ax2.plot(cum_rewards, label=\"avg rewards\")\n",
        "  ax2.set_title(\"Cummulative Rewards\")\n",
        "\n",
        "  ax3 = fig.add_subplot(122)\n",
        "  ax3.bar([i for i in range(len(chosen_arms))], chosen_arms, label=\"chosen arms\")\n",
        "  ax3.set_title(\"Chosen Actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiW4JZ5yrmKd"
      },
      "source": [
        "## **Creating Environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPnkfiuurpK_"
      },
      "outputs": [],
      "source": [
        "class Env:\n",
        "    def __init__(self, reward_prob, rewards):\n",
        "        self.reward_prob = reward_prob\n",
        "        self.rewards = rewards\n",
        "        self.k_arms = len(rewards)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"K_arms: {self.k_arms}\\nReward Probabilities: {self.reward_prob} Rewards: {self.rewards}\"\n",
        "\n",
        "    def step(self, arm):\n",
        "        if np.random.random() < self.reward_prob[arm]:\n",
        "            return self.rewards[arm]\n",
        "        else:\n",
        "            return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQx6_tCNvBBn"
      },
      "source": [
        "## **Instantiating the environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvF_M4eLvFTD"
      },
      "outputs": [],
      "source": [
        "reward_prob = np.array([0.01, 1.0, 0.65, 0.99, 0.65, 1.0])\n",
        "rewards = np.array([95.0, 0.0, 25.5, 10.05, 5.45, 2.50])\n",
        "\n",
        "env = Env(reward_prob, rewards)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWyzYRg9wQS4"
      },
      "source": [
        "## **Performing a selective action**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpFDJuYEwXGe",
        "outputId": "3693e281-0300-484f-bc39-c50377e6b6b6"
      },
      "outputs": [],
      "source": [
        "selected_arm = 0\n",
        "[env.step(selected_arm) for _ in range(10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LcqmTA7xBAb"
      },
      "source": [
        "## **Balancing Exploration and Exploitation with epsilon greedy algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff8x5uwHxSLF"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedyAgent:\n",
        "    def __init__(self, env, max_iters=200, eps=(lambda t: 0.1)):\n",
        "        self.env = env\n",
        "        self.iters = max_iters\n",
        "        self.eps = eps\n",
        "\n",
        "\n",
        "    def act(self):\n",
        "        q_values = np.zeros(self.env.k_arms)\n",
        "        arm_rewards = np.zeros(self.env.k_arms)\n",
        "        arm_counts = np.zeros(self.env.k_arms)\n",
        "        eps_history = []\n",
        "\n",
        "        rewards = [0]\n",
        "        cum_rewards = [0]\n",
        "\n",
        "        for i in tqdm(range(self.iters)):\n",
        "            eps_history.append(self.eps(i))\n",
        "\n",
        "            if np.random.random() < eps_history[-1]:\n",
        "                arm = np.random.choice(self.env.k_arms)\n",
        "            else:\n",
        "                arm = np.argmax(q_values)\n",
        "\n",
        "            \n",
        "            reward = self.env.step(arm)\n",
        "\n",
        "            arm_rewards[arm] += reward\n",
        "            arm_counts[arm] += 1\n",
        "            q_values[arm] = arm_rewards[arm] / arm_counts[arm]\n",
        "\n",
        "            rewards.append(reward)\n",
        "            cum_rewards.append(sum(rewards) / len(rewards))\n",
        "\n",
        "        \n",
        "        plt.plot(eps_history)\n",
        "\n",
        "        return {\n",
        "            \"rewards\": rewards,\n",
        "            \"cum_rewards\": cum_rewards,\n",
        "            \"arms\": arm_counts\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlZwXleMzzNK"
      },
      "source": [
        "## **Implementing Epsilon greedy algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "p2PJ1SAdz3vx",
        "outputId": "14587bad-9819-4a98-9127-63cd7e32fd0a"
      },
      "outputs": [],
      "source": [
        "def decay_eps(t, c=0.01):\n",
        "    return np.exp(-c * t) # Epsilon decay schedule \n",
        "\n",
        "eps_greedy_agent_1 = EpsilonGreedyAgent(env, max_iters=2000, eps=lambda t: decay_eps(t, 0.01))\n",
        "eps_greedy_agent_2 = EpsilonGreedyAgent(env, max_iters=2000, eps=lambda t: decay_eps(t, 0.05))\n",
        "\n",
        "history_1 = eps_greedy_agent_1.act()\n",
        "history_2 = eps_greedy_agent_2.act()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlrC12M_2yVs"
      },
      "source": [
        "## **Mapping plot for Epsilon greedy algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "zkW3zlAG23MW",
        "outputId": "81dcf3a0-ea2d-4189-d944-610ea5ab9f16"
      },
      "outputs": [],
      "source": [
        "plot_history(history_1)\n",
        "plot_history(history_2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CSF317 - Aug 31 Lesson 1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
