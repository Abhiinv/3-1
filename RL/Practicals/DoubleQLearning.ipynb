{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0080240755212109"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(-0.1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxBiasEnv(gym.Env):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        self.observation_space = gym.spaces.Discrete(3)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.state == 0:\n",
    "            if action == 0:  # Going Right\n",
    "                self.state = 2\n",
    "                return self.state, 0, True, {}\n",
    "\n",
    "            elif action == 1:\n",
    "                self.state = 1\n",
    "                return self.state, 0, False, {}\n",
    "\n",
    "        elif self.state == 1: # Going Left\n",
    "            self.state = 2\n",
    "            return self.state, np.random.normal(-0.1, 1), True, {}\n",
    "\n",
    "        else:\n",
    "            return self.state, 0, True, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        \n",
    "        return self.state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MaxBiasEnv() # gym.make(\"FrozenLake-v1\")\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    def __init__(self, n_states, n_actions, lr, gamma, epsilon=0.1) -> None:\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.__class__.__name__}\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def init_Q(self):  ## set everything equal to zero for all state n action pairs\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def choose_action(self, state):  ## choose action using epsilon greedy algorithm\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, n_states, n_actions, lr, gamma, epsilon) -> None:\n",
    "        super(QLearningAgent, self).__init__(n_states, n_actions, lr, gamma, epsilon)\n",
    "\n",
    "        self.Q = {}  ##Q table is initialized to empty dictionary\n",
    "        self.init_Q()  ##specific function to initialize the Q table\n",
    "\n",
    "        self.steps = 0\n",
    "\n",
    "    def init_Q(self):  ## set everything equal to zero for all state n action pairs\n",
    "        for state in range(self.n_states):\n",
    "            self.Q[state] = np.zeros(self.n_actions)\n",
    "\n",
    "    def choose_action(self, state):  ## choose action using epsilon greedy algorithm\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.Q[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.steps += 1\n",
    "\n",
    "        self.Q[state][action] = self.Q[state][action] + self.lr * (\n",
    "            (reward + self.gamma * np.amax(self.Q[state_])) - self.Q[state][action]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent(Agent):\n",
    "    def __init__(self, n_states, n_actions, lr, gamma, epsilon) -> None:\n",
    "        super(DoubleQLearningAgent, self).__init__(\n",
    "            n_states, n_actions, lr, gamma, epsilon\n",
    "        )\n",
    "\n",
    "        self.Q1 = {}  ##Q table is initialized to empty dictionary\n",
    "        self.Q2 = {}  ##Q table is initialized to empty dictionary\n",
    "\n",
    "        self.init_Q()  ##specific function to initialize the Q table\n",
    "        self.steps = 0\n",
    "\n",
    "    def init_Q(self):  ## set everything equal to zero for all state n action pairs\n",
    "        for state in range(self.n_states):\n",
    "            self.Q1[state] = np.zeros(self.n_actions)\n",
    "            self.Q2[state] = np.zeros(self.n_actions)\n",
    "\n",
    "    def choose_action(self, state):  ## choose action using epsilon greedy algorithm\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = np.random.choice(self.n_actions)\n",
    "        else:\n",
    "            action = np.argmax(self.Q1[state] + self.Q2[state])\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.steps += 1\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            actor_action = np.argmax(self.Q1[state_])\n",
    "            self.Q1[state][action] += self.lr * (\n",
    "                reward\n",
    "                + self.gamma * self.Q2[state_][actor_action]\n",
    "                - self.Q1[state][action]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            actor_action = np.argmax(self.Q2[state_])\n",
    "            self.Q2[state][action] += self.lr * (\n",
    "                reward\n",
    "                + self.gamma * self.Q1[state_][actor_action]\n",
    "                - self.Q2[state][action]\n",
    "            )\n",
    "\n",
    "        self.decrement_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FAFODoubleQLearningAgent(DoubleQLearningAgent):\n",
    "    def __init__(self, n_states, n_actions, lr, gamma, epsilon) -> None:\n",
    "        super(FAFODoubleQLearningAgent, self).__init__(\n",
    "            n_states, n_actions, lr, gamma, epsilon\n",
    "        )\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "\n",
    "        self.steps += 1\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            actor_action = np.argmax(self.Q2[state_])\n",
    "            self.Q1[state][action] += self.lr * (\n",
    "                reward\n",
    "                + self.gamma * self.Q1[state_][actor_action]\n",
    "                - self.Q1[state][action]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            actor_action = np.argmax(self.Q1[state_])\n",
    "            self.Q2[state][action] += self.lr * (\n",
    "                reward\n",
    "                + self.gamma * self.Q2[state_][actor_action]\n",
    "                - self.Q2[state][action]\n",
    "            )\n",
    "\n",
    "        self.decrement_epsilon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_states\": env.observation_space.n,\n",
    "    \"n_actions\": env.action_space.n,\n",
    "    \"lr\": 0.1,\n",
    "    \"gamma\": 1,\n",
    "    \"epsilon\": 0.1,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_agent(agent, steps = 100):\n",
    "    num_left = 0\n",
    "    num_total = 0\n",
    "    for t in range(steps):\n",
    "        env.seed(t)\n",
    "        state, done = env.reset(), False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "\n",
    "            num_left += (action == 1)\n",
    "            num_total += 1\n",
    "\n",
    "            state_, reward, done, info = env.step(action)\n",
    "            state = state_\n",
    "\n",
    "    return num_left /  num_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 510.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# agent = QLearningAgent(**config) # 7.567%\n",
    "# agent = DoubleQLearningAgent(**config) # 5.638%\n",
    "agent = FAFODoubleQLearningAgent(**config) # 23.53%\n",
    "\n",
    "NUM_EPISODES = 300 # int(5e4)\n",
    "writer = SummaryWriter(comment=f\"{agent}-{int(time.time())}\", flush_secs=5)\n",
    "\n",
    "for episode_num in tqdm(range(NUM_EPISODES)):\n",
    "    state, done = env.reset(), False\n",
    "    score = 0\n",
    "\n",
    "    num_left = 0\n",
    "    num_total = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        num_left += (action == 1)\n",
    "        num_total += 1\n",
    "\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        agent.learn(state, action, reward, state_)\n",
    "        score += reward\n",
    "        state = state_\n",
    "\n",
    "        writer.add_scalar(\"epsilon\", agent.epsilon, global_step=agent.steps)\n",
    "\n",
    "    pct_left = eval_agent(agent) * 100\n",
    "\n",
    "    writer.add_scalar(\"reward\", score, global_step=episode_num)\n",
    "    writer.add_scalar(\"pct_left\", pct_left, global_step=episode_num)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47a315ad2c746ccae36dcb639cdfe5a64496b51c60959dc41c355f9e56d2b9a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
