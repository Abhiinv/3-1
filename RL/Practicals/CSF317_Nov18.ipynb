{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning in Windy Gridworld Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from gym.envs.toy_text import discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP, RIGHT, DOWN, LEFT = 0, 1, 2, 3\n",
    "\n",
    "\n",
    "class WindyGridworldEnv(discrete.DiscreteEnv):\n",
    "    def __init__(self):\n",
    "        self.shape = (7, 10)\n",
    "        nS = self.shape[0] * self.shape[1]\n",
    "        nA = 4\n",
    "\n",
    "        winds = np.zeros(self.shape)\n",
    "        winds[:, (3, 4, 5, 8)] = 1\n",
    "        winds[:, (6, 7)] = 2\n",
    "\n",
    "        self.goal = (3, 7)\n",
    "\n",
    "        isd = np.zeros(nS)\n",
    "        isd[np.ravel_multi_index((3, 0), self.shape)] = 1.0\n",
    "\n",
    "        P = {}\n",
    "        for s in range(nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = {a: [] for a in range(nA)}\n",
    "            P[s][UP] = self._calc_transition_probabilities(\n",
    "                position, [-1, 0], winds\n",
    "            )  # -1 => Moves row up, 0 => Same Col\n",
    "            P[s][RIGHT] = self._calc_transition_probabilities(\n",
    "                position, [0, 1], winds\n",
    "            )  # -1 => Moves row up, 0 => Same Col\n",
    "            P[s][DOWN] = self._calc_transition_probabilities(\n",
    "                position, [1, 0], winds\n",
    "            )  # -1 => Moves row up, 0 => Same Col\n",
    "            P[s][LEFT] = self._calc_transition_probabilities(\n",
    "                position, [0, -1], winds\n",
    "            )  # -1 => Moves row up, 0 => Same Col\n",
    "\n",
    "        super(WindyGridworldEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def _calc_transition_probabilities(self, pos, delta, winds):\n",
    "        new_pos = (\n",
    "            np.array(pos) + np.array(delta) + np.array([-1, 0]) * winds[tuple(pos)]\n",
    "        )  # isn't pos already a tuple??\n",
    "        new_pos = self._limit_coordinates(new_pos).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_pos), self.shape)\n",
    "\n",
    "        is_done = tuple(new_pos) == self.goal\n",
    "\n",
    "        return [(1.0, new_state, -1.0, is_done)] # prob, new_stae, reward, done\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        return np.clip(coord, (0, 0), np.array(self.shape) - 1)\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        outfile = sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            pos = np.unravel_index(s, self.shape)\n",
    "\n",
    "            if self.s == s:\n",
    "                output = \" s \"\n",
    "            elif pos == self.goal:\n",
    "                output = \" G \"\n",
    "\n",
    "            else:\n",
    "                output = \" _ \"\n",
    "\n",
    "            if pos[1] == 0:\n",
    "                output = output.lstrip()\n",
    "            if pos[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip()\n",
    "                output += \"\\n\"\n",
    "\n",
    "            outfile.write(output)\n",
    "        outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = WindyGridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "s  _  _  _  _  _  _  G  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, -1.0, False, {'prob': 1.0})\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  s  _  _  _  _  _  G  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "_  _  _  _  _  _  _  _  _  _\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(env.step(1))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It is Q-Learning Time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, nA, epsilon):\n",
    "    probs = np.ones(nA) * epsilon / nA # should this not be nA - 1?\n",
    "\n",
    "    best_action = np.argmax(Q[state])\n",
    "    probs[best_action] += 1 - epsilon\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning(env: gym.Env, episodes: int, learning_rate: float, gamma: float, epsilon: float):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    x = np.arange(episodes)\n",
    "    y = np.zeros(episodes)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "\n",
    "        for step in range(int(1e5)):\n",
    "            probs = epsilon_greedy_policy(Q, state, env.action_space.n, epsilon)\n",
    "            action = np.random.choice(np.arange(env.action_space.n), p=probs)\n",
    "\n",
    "            next_state, reward, done, _  = env.step(action)\n",
    "\n",
    "            Q[state][action] += learning_rate * ((reward + gamma * np.max(Q[next_state])) - Q[state][action])\n",
    "\n",
    "            if done:\n",
    "                y[ep] = step\n",
    "                break\n",
    "        \n",
    "            state = next_state\n",
    "\n",
    "    return x, y, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x, y, Q = Q_learning(env, 1000, 0.01, .99, .05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18, 5))\n",
    "# plt.plot(x, y)\n",
    "\n",
    "# plt.xlabel(\"episodes\")\n",
    "# plt.ylabel(\"steps required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, done = env.reset(), False\n",
    "# env.render()\n",
    "# rewards = []\n",
    "\n",
    "# while not done:\n",
    "#     action = np.argmax(Q[state])\n",
    "#     # print(f\"Taking Action: {action}\")\n",
    "#     # actions.append(action)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     # env.render()\n",
    "\n",
    "#     rewards.append(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(\n",
    "        self, lr, gamma, n_actions, n_states, eps_start, eps_end, eps_dec\n",
    "    ) -> None:\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.n_actions = n_actions\n",
    "        self.n_states = n_states\n",
    "        self.epsilon = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.eps_dec = eps_dec\n",
    "\n",
    "        self.Q = {s: np.random.randn(self.n_actions) for s in range(self.n_states)}\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def _epsilon_greedy_policy(self, state):\n",
    "        probs = (\n",
    "            np.ones(self.n_actions) * self.epsilon / self.n_actions\n",
    "        )  # should this not be nA - 1?\n",
    "\n",
    "        best_action = np.argmax(self.Q[state])\n",
    "        probs[best_action] += 1 - self.epsilon\n",
    "\n",
    "        return probs\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        probs = self._epsilon_greedy_policy(state)\n",
    "        return np.random.choice(np.arange(self.n_actions), p=probs)\n",
    "\n",
    "    def eps_udpate(self):\n",
    "        self.epsilon = max(self.epsilon - self.eps_dec, self.eps_end)\n",
    "\n",
    "    def learn(self, state, action, reward, state_):\n",
    "        self.total_steps += 1\n",
    "        self.Q[state][action] += self.lr * (\n",
    "            (reward + self.gamma * np.amax(self.Q[state_])) - self.Q[state][action]\n",
    "        )\n",
    "\n",
    "        self.eps_udpate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 503/50000 [00:00<00:19, 2534.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[8]: [-1.05641711 -1.25800186 -0.66419868  0.27465538]\n",
      "Q[4]: [-0.85861905  0.17171838 -0.79147759 -0.96480823]\n",
      "Q[9]: [-1.105696   -0.101317    1.08191194  0.63386408]\n",
      "Q[6]: [-0.29655543  0.38539145  2.03970749  0.05484432]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1306/50000 [00:00<00:18, 2572.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[3]: [-0.073238   -0.99465814  1.48234467 -0.60721787]\n",
      "Q[4]: [-0.85741122  0.43183953 -0.788412   -0.95997756]\n",
      "Q[4]: [-0.85611448  0.47713673 -0.78644296 -0.95997756]\n",
      "Q[0]: [-0.31364068  0.94795345 -0.00097085 -0.93631903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 1830/50000 [00:00<00:18, 2575.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-3.13640683e-01  8.94859904e-01 -1.01262660e-04 -9.32652649e-01]\n",
      "Q[1]: [ 0.90075047  0.19119475 -0.6507247   0.18386366]\n",
      "Q[0]: [-3.12446141e-01  8.38153126e-01 -1.01262660e-04 -9.27263977e-01]\n",
      "Q[0]: [-3.11314722e-01  8.24912638e-01  5.40826491e-04 -9.27263977e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2597/50000 [00:01<00:19, 2442.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[9]: [-1.0978026  -0.10088687  1.19257495  0.63424034]\n",
      "Q[4]: [-0.8480244   0.72214814 -0.78150582 -0.94931054]\n",
      "Q[0]: [-3.10191958e-01  8.18976518e-01  5.40826491e-04 -9.25526463e-01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3096/50000 [00:01<00:19, 2460.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-3.09117778e-01  8.21262270e-01  5.40826491e-04 -9.23663232e-01]\n",
      "Q[1]: [ 0.95194483  0.1928004  -0.64551717  0.18632446]\n",
      "Q[8]: [-1.04666679 -1.25194645 -0.65989138  0.69706452]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3590/50000 [00:01<00:19, 2351.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[10]: [-2.18676118  0.27550662 -0.9106375   0.82888905]\n",
      "Q[0]: [-0.30682156  0.85722435  0.00482537 -0.91490773]\n",
      "Q[0]: [-0.30682156  0.86639073  0.00567818 -0.91302903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4090/50000 [00:01<00:18, 2425.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[13]: [-1.65393187 -0.66707895  1.90816053 -0.37837019]\n",
      "Q[8]: [-1.04293853 -1.24946448 -0.65834734  0.79554199]\n",
      "Q[0]: [-0.29974252  0.89419423  0.00936031 -0.91302903]\n",
      "Q[0]: [-0.29974252  0.90151859  0.0102528  -0.91302903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 4883/50000 [00:01<00:18, 2506.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[1]: [ 0.99983851  0.19786819 -0.63448316  0.18698036]\n",
      "Q[8]: [-1.03731521 -1.24740953 -0.65644736  0.86327634]\n",
      "Q[4]: [-0.82978938  0.95377232 -0.76919167 -0.92964636]\n",
      "Q[0]: [-0.29732731  0.9371232   0.01770274 -0.9055868 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 5397/50000 [00:02<00:17, 2513.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[4]: [-0.82806332  0.97138369 -0.76749065 -0.92964636]\n",
      "Q[0]: [-0.29610044  0.95157628  0.01770274 -0.9055868 ]\n",
      "Q[4]: [-0.82806332  0.98711526 -0.76749065 -0.92775348]\n",
      "Q[1]: [ 1.02517386  0.19995095 -0.62422187  0.1937963 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6196/50000 [00:02<00:16, 2614.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-0.29485017  0.97274706  0.02349259 -0.90002303]\n",
      "Q[4]: [-0.82628459  1.01593834 -0.76210986 -0.9258453 ]\n",
      "Q[4]: [-0.82628459  1.0246435  -0.76210986 -0.9258453 ]\n",
      "Q[9]: [-1.08336073 -0.09492182  1.26572539  0.63478672]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 6716/50000 [00:02<00:17, 2541.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[6]: [-0.29559783  0.38618668  1.61417536  0.05605925]\n",
      "Q[1]: [ 1.04712454  0.20577563 -0.62241703  0.19462416]\n",
      "Q[14]: [1.46846653 0.80570792 0.15331257 0.7162826 ]\n",
      "Q[0]: [-0.28713462  1.01576196  0.03039346 -0.89232285]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 7532/50000 [00:02<00:15, 2659.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[9]: [-1.08058271 -0.09202535  1.2715346   0.63512225]\n",
      "Q[0]: [-0.28450928  1.02600172  0.03340424 -0.8884429 ]\n",
      "Q[0]: [-0.28316574  1.0312837   0.03544192 -0.88463353]\n",
      "Q[8]: [-1.02970682 -1.2269118  -0.6479003   1.05731113]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8061/50000 [00:03<00:16, 2582.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[4]: [-0.82256594  1.08220793 -0.74932898 -0.91964187]\n",
      "Q[4]: [-0.8169459   1.08601172 -0.74932898 -0.91964187]\n",
      "Q[0]: [-0.27919471  1.04997011  0.03644367 -0.88271994]\n",
      "Q[4]: [-0.8131794   1.0942077  -0.74572927 -0.9176425 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 8837/50000 [00:03<00:16, 2522.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-0.27651127  1.0571728   0.03851266 -0.87886989]\n",
      "Q[8]: [-1.02549721 -1.22033992 -0.64599615  1.09185641]\n",
      "Q[1]: [ 1.08909895  0.21744988 -0.605704    0.19713591]\n",
      "Q[8]: [-1.02370923 -1.21538788 -0.64599615  1.10254425]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 9341/50000 [00:03<00:16, 2459.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-0.26847441  1.07125312  0.04059199 -0.87497465]\n",
      "Q[0]: [-0.26847441  1.07463165  0.04271207 -0.87497465]\n",
      "Q[1]: [ 1.09820411  0.21841305 -0.6037844   0.19913839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 9836/50000 [00:03<00:16, 2430.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[14]: [1.43641101 0.80697174 0.1535462  0.7162826 ]\n",
      "Q[8]: [-1.02158141 -1.21341001 -0.64234591  1.12433915]\n",
      "Q[0]: [-0.26713265  1.08548982  0.04791628 -0.865258  ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 10333/50000 [00:04<00:16, 2449.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[4]: [-0.80371484  1.12689001 -0.73100277 -0.90739143]\n",
      "Q[0]: [-0.26302538  1.0907229   0.0499946  -0.86329717]\n",
      "Q[4]: [-0.79995067  1.13151931 -0.73100277 -0.90131982]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11125/50000 [00:04<00:14, 2596.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[0]: [-0.26302538  1.09569318  0.05315693 -0.86329717]\n",
      "Q[4]: [-0.79806565  1.13616001 -0.7291881  -0.89721693]\n",
      "Q[1]: [ 1.1155146   0.22796343 -0.5932408   0.20183231]\n",
      "Q[8]: [-1.0112311  -1.21341001 -0.63906098  1.14761285]\n",
      "Q[0]: [-0.25755075  1.10455353  0.05735951 -0.85741161]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 11903/50000 [00:04<00:14, 2555.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[4]: [-0.79617863  1.14372756 -0.72545684 -0.89522903]\n",
      "Q[1]: [ 1.12147526  0.22891609 -0.59146694  0.20294669]\n",
      "Q[4]: [-0.79041326  1.14665998 -0.72358892 -0.88909156]\n",
      "Q[0]: [-0.25075621  1.11129836  0.06265499 -0.85350622]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 12425/50000 [00:04<00:14, 2522.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[1]: [ 1.12634966  0.22891609 -0.58235294  0.20406021]\n",
      "Q[8]: [-1.00908289 -1.20869633 -0.63906098  1.15945636]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11802/2473315408.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11802/1409029826.py\u001b[0m in \u001b[0;36mchoose_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon_greedy_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meps_udpate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "agent = Agent(\n",
    "    lr=0.001,\n",
    "    gamma=0.99,\n",
    "    n_actions=env.action_space.n,\n",
    "    n_states=env.observation_space.n,\n",
    "    eps_start=0.1,\n",
    "    eps_end=0.01,\n",
    "    eps_dec=1e-3,\n",
    ")\n",
    "\n",
    "ngames = int(5e4)\n",
    "win_pct = []\n",
    "scores = []\n",
    "\n",
    "for i in tqdm(range(ngames)):\n",
    "    state, done = env.reset(), False\n",
    "    score = 0\n",
    "\n",
    "    while not done:\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        agent.learn(state, action, reward, next_state)\n",
    "\n",
    "        if agent.total_steps % 1000 == 0:\n",
    "            print(f\"Q[{state}]: {agent.Q[state]}\")\n",
    "\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "    scores.append(score)\n",
    "    if i % 100 == 0:\n",
    "        average = np.mean(scores[-100:])\n",
    "        win_pct.append(average)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(scores[::100])\n",
    "plt.plot(win_pct)\n",
    "plt.legend([\"Scores\", \"Win Percentages\"])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47a315ad2c746ccae36dcb639cdfe5a64496b51c60959dc41c355f9e56d2b9a3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('rl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
